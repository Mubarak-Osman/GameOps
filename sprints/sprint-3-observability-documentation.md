
---
### Sprint 3
---

#### Zeitraum

<table>
  <thead>
    <tr>
      <th style="background-color:#f2f2f2;">Period</th>
      <th style="background-color:#f2f2f2;">Task</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="color:#2E86C1;">05.12.2025 ‚Äì 14.12.2025</td>
      <td style="color:#117A65;">GameOps Observability & Documentation</td>
    </tr>

  </tbody>
</table>

---
#### Sprintziel

Ziel dieses Sprints ist es, die GameOps-Anwendung besser beobachtbar und stabil zu machen, indem wir ein Monitoring-Tool ausw√§hlen und implementieren, Ressourcenlimits und Liveness/Readiness-Probes konfigurieren, sowie die Deployment-Architektur dokumentieren und eine Pr√§sentation/Demo vorbereiten.  

Dieser Sprint stellt sicher, dass:

- Pods √ºberwacht werden k√∂nnen und Leistungskennzahlen sichtbar sind.  
- Ressourcen effizient zugewiesen sind und die Anwendung fehlertolerant l√§uft.  
- Alle Setup-Schritte, Architektur und CI/CD-Prozesse klar dokumentiert sind.  
- Eine Pr√§sentation mit Live-Demo und Visualisierungen des Monitoring-Tools bereitsteht.

---
#### GameOps Observability & Documentation

![Monitoring](../images/monitoring.png)

---
##### üü£ User Story 15: **Monitoring-Tools vergleichen und ausw√§hlen** <a name="user-story-15"></a>  

**Als** DevOps Engineer  
**m√∂chte ich** Lens und Grafana + Prometheus vergleichen  
**damit** ich das geeignetste Tool f√ºr Echtzeit-Monitoring von Pods, Logs und Performance-Metriken ausw√§hlen kann.  

**Akzeptanzkriterien:**  

- Vergleichskriterien werden definiert (Echtzeit-Logs, Dashboards, Metriken).  
- Entscheidung f√ºr ein Tool dokumentiert.  
- Entscheidung basiert auf Stabilit√§t, √úbersichtlichkeit und Einsatz im lokalen Minikube-Cluster.  
---
##### üü£ User Story 16: **Monitoring-Tool implementieren und √ºberwachen** <a name="user-story-16"></a>  

**Als** DevOps Engineer  
**m√∂chte ich** das ausgew√§hlte Monitoring-Tool einrichten  
**damit** Pods, Logs und Ressourcen in Echtzeit √ºberwacht werden k√∂nnen.  

**Akzeptanzkriterien:**  

- Tool ist installiert und konfiguriert (Lens oder Grafana + Prometheus).  
- Dashboards oder Visualisierungen zeigen Status, Logs und Metriken der Pods.  
- Alerts oder Hinweise bei Fehlfunktionen werden getestet.  

---
##### üü£ User Story 17: **Setup und Architektur dokumentieren** <a name="user-story-17"></a>  

**Als** Entwickler / DevOps Engineer  
**m√∂chte ich** die Deployment-Schritte, Architektur und CI/CD-Workflow dokumentieren  
**damit** andere Teammitglieder die Umgebung verstehen und reproduzieren k√∂nnen.  

**Akzeptanzkriterien:**  

- Alle Schritte zur Installation und Konfiguration sind dokumentiert.  
- Architekturdiagramme oder Schema der Cluster-Ressourcen sind vorhanden.  
- CI/CD-Pipeline mit Build, Push und Deployment ist beschrieben.
---
##### üü£ User Story 18: **Pr√§sentation und Demo vorbereiten** <a name="user-story-18"></a>  

**Als** Entwickler / DevOps Engineer  
**m√∂chte ich** eine Pr√§sentation und Demo erstellen  
**damit** der Fortschritt des Projekts, die Monitoring-Visualisierungen und die Cluster-Performance vorgestellt werden k√∂nnen.  

**Akzeptanzkriterien:**  

- Pr√§sentation enth√§lt Screenshots oder Dashboards des Monitoring-Tools.  
- Live-Demo des Deployments auf Minikube m√∂glich.  
- Kernpunkte von Monitoring, Optimierung und Architektur sind verst√§ndlich dargestellt. 
---

| Nr. | Bereich                     | User Story                                                                 | Status   |
|-----|-----------------------------|---------------------------------------------------------------------------|----------|
| 15  | Monitoring Tool Evaluation   | [Monitoring-Tools vergleichen und ausw√§hlen](#user-story-15)              | ‚úÖ Done |
| 16  | Monitoring Implementation    | [Monitoring-Tool implementieren und √ºberwachen](#user-story-16)           | ‚úÖ Done |
| 17  | Documentation               | [Setup und Architektur dokumentieren](#user-story-17)                     | ‚úÖ Done |
| 18  | Presentation & Demo         | [Pr√§sentation und Demo vorbereiten](#user-story-18)                       | ‚úÖ Done |

---

#### üèÅ Sprint Review

---

##### ‚úÖ Was wurde erreicht?
- Lens als Monitoring-Tool f√ºr Minikube erfolgreich ausgew√§hlt und implementiert.  
- Echtzeit-√úberwachung von Pods, Logs, CPU- und Memory-Metriken eingerichtet.  
- Ressourcenlimits, Liveness- und Readiness-Probes f√ºr GameOps-Pods konfiguriert.  
- Deployment-Architektur und CI/CD-Workflow vollst√§ndig dokumentiert.  
- Pr√§sentation mit Screenshots der Dashboards und Live-Demo auf Minikube vorbereitet.  
- Teammitglieder konnten die Umgebung dank Dokumentation problemlos reproduzieren.  

---
##### ‚ö†Ô∏è Herausforderungen
- Auswahl des Monitoring-Tools erforderte Abw√§gung zwischen Echtzeitf√§higkeit (Lens) und Langzeit-Analysen (Grafana/Prometheus).  
- Liveness- und Readiness-Probes mussten mehrfach angepasst werden, um False-Positives zu vermeiden.  
- Bei Live-Demo traten initial kleine Verz√∂gerungen bei Pod-Status und Log-Anzeige auf.  
- Dokumentation der genauen Installations- und Konfigurationsschritte war zeitaufwendig, um reproduzierbar zu sein.

---
##### üéì Lessons Learned
- Lens eignet sich hervorragend f√ºr lokale Test- und Entwicklungscluster, schnelle Fehleranalyse und Debugging.  
- Ressourcenlimits und Probes verbessern die Stabilit√§t der Anwendung und verhindern CrashLoops.  
- Dokumentation und Visualisierung sind entscheidend, um die Transparenz f√ºr Team und Stakeholder zu erh√∂hen.  
- Vorbereitung einer Live-Demo erfordert fr√ºhzeitiges Testen der Cluster-Performance und Dashboards.

---
#### üîç Sprint Retrospektive

---

##### ‚úÖ Was lief gut?
- Monitoring-Tool konnte schnell evaluiert, ausgew√§hlt und implementiert werden.  
- Echtzeit-Metriken und Logs erm√∂glichten schnelle Identifikation von Problemen.  
- Dokumentation und Pr√§sentation erm√∂glichten eine klare Kommunikation des Projektfortschritts.  
- Team konnte selbstst√§ndig Tests und Dashboards nachvollziehen.
---

##### ‚ö†Ô∏è Was lief nicht gut?
- Feinjustierung von Probes und Ressourcenlimits dauerte l√§nger als geplant.  
- Live-Demo auf Minikube zeigte gelegentlich verz√∂gerte Updates in Lens.  
- Dokumentation musste mehrfach angepasst werden, um verst√§ndlich und reproduzierbar zu sein.

---

##### üöÄ Verbesserungsm√∂glichkeiten
- Standardisierte Vorlage f√ºr Monitoring-Setup inkl. Probes, Ressourcenlimits und Dashboards einf√ºhren.  
- Automatisierte Checks f√ºr Deployment- und Pod-Status im Cluster implementieren.  
- Regelm√§√üige Tests f√ºr Live-Demo und Dashboard-Darstellung einplanen.  
- Feedback-Runden mit Teammitgliedern fr√ºhzeitig in die Dokumentation einbeziehen.